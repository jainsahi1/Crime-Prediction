#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
# Impute missing values using SimpleImputer
from sklearn.impute import SimpleImputer


# In[4]:


# Load the dataset
data = pd.read_csv("Crime.csv")

print(data.head())

print(data.info())


# In[5]:


# Check for missing values
print(data.isnull().sum())

# Option 1: Fill missing values with zero for columns where it's appropriate (e.g., crime counts)
data.fillna(0, inplace=True)

# Option 2: Drop rows with missing values if they are few and donâ€™t impact analysis
# data.dropna(inplace=True)


# In[6]:


# Drop non-informative columns for modeling
selected_data = data.drop(columns=['STATE/UT', 'DISTRICT'])

# Calculate correlation to check for highly correlated features
correlation_matrix = selected_data.corr()

# Display correlation matrix to decide if any features can be dropped
print(correlation_matrix)


# In[7]:


# Standardize column names by converting to lowercase and removing leading/trailing spaces
#selected_data.columns = selected_data.columns.str.lower().str.strip()

# Calculate YoY change for each crime type
for crime_type in data.columns[2:]:  # Skipping 'YEAR'
    data[f'{crime_type}_yoy_change'] = data.groupby('DISTRICT')[crime_type].pct_change() * 100


# In[8]:


# Calculate rolling mean (3-year window) for each crime type
for crime_type in data.columns[2:]:
    data[f'{crime_type}_3yr_avg'] = data.groupby('DISTRICT')[crime_type].rolling(window=3).mean().reset_index(level=0, drop=True)


# In[11]:


# Binary target for increase (1) or decrease (0) in total IPC crimes
data['total_ipc_crimes_increase'] = (data['TOTAL IPC CRIMES'].diff() > 0).astype(int)


# In[15]:


# Example: Theft to Burglary ratio
data['BURGLARY'] = data['THEFT'] / (data['BURGLARY'] + 1)


# In[16]:


# Drop rows with NaN values after feature engineering
final_data = data.dropna()

# Save the final dataset
final_data.to_csv("final_crime_data.csv", index=False)


# In[19]:


# One-Hot Encode the 'DISTRICT' column
encoded_data = pd.get_dummies(data, columns=['DISTRICT'], prefix='DISTRICT')

# Display the first few rows of the encoded dataset
print(encoded_data.head())


# In[22]:


# Assuming 'encoded_data' is the DataFrame after encoding, and 'target' is the target column name
# Define your target variable. For example, let's assume we're predicting increase/decrease in 'TOTAL IPC CRIMES'
encoded_data['crime_increase'] = (encoded_data['TOTAL IPC CRIMES'].diff() > 0).astype(int)

# Drop columns that are not needed for training
X = encoded_data.drop(columns=['TOTAL IPC CRIMES', 'crime_increase', 'YEAR'])
y = encoded_data['crime_increase']

# Before splitting into train/test, identify and encode object (string) columns
for col in X.select_dtypes(include=['object']).columns:
    # Create a LabelEncoder for each categorical column
    le = LabelEncoder()
    # Fit and transform the column in the entire dataset (X)
    X[col] = le.fit_transform(X[col])

# Replace infinite values with NaN
X.replace([np.inf, -np.inf], np.nan, inplace=True)
imputer = SimpleImputer(strategy='mean')  # You can change the strategy if needed
X = imputer.fit_transform(X)

# Split data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Optional: Further split the training data into training and validation sets (if needed)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 60% train, 20% val, 20% test

# Display the shapes of the datasets
print("Training set shape:", X_train.shape)
print("Validation set shape:", X_val.shape)
print("Testing set shape:", X_test.shape)


# In[28]:


# Initialize the Logistic Regression model
log_reg = LogisticRegression(max_iter=200, random_state=42)

# Train the model on the training set
log_reg.fit(X_train, y_train)

# Predict on the test set
y_pred = log_reg.predict(X_test)




# In[29]:


# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))


# In[31]:


# Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)  # Set max_depth to control overfitting

# Train the model
dt_model.fit(X_train, y_train)

# Predict on the test set
y_pred_dt = dt_model.predict(X_test)




# In[32]:


# Evaluate the model
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Classification Report for Decision Tree:\n", classification_report(y_test, y_pred_dt))


# In[33]:


# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)  # Set max_depth and n_estimators

# Train the model
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)


# In[34]:


# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report for Random Forest:\n", classification_report(y_test, y_pred_rf))


# In[35]:


xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

# Predict on the test set
y_pred_xgb = xgb_model.predict(X_test)



# In[36]:


# Evaluate the model
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report for XGBoost:\n", classification_report(y_test, y_pred_xgb))
